{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"eventLogging/local-1588781816130\"\n",
    "spark = SQLContext(sc)\n",
    "\n",
    "df=sqlConf.read.json(path)\n",
    "\n",
    "df.createOrReplaceTempView(\"Events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of events So far = 81\n"
     ]
    }
   ],
   "source": [
    "# The Number of events so far\n",
    "print(f'The number of events So far = {df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(App Name='PySparkShell')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"App Name\").dropna().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct events So far = 11\n"
     ]
    }
   ],
   "source": [
    "dist_events=df.groupBy('Event').count()\n",
    "print(f'The number of distinct events So far = {dist_events.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list=[]\n",
    "event_timestamp=dict()\n",
    "for event in dist_events.collect():\n",
    "    \n",
    "    event_dict=event.asDict()\n",
    "    \n",
    "    event_list.append(event_dict.['Event'])\n",
    "    \n",
    "    event_timestamp[event_dict.['Event']]=event_dict.['Event']\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_event=[]\n",
    "\n",
    "for event in event_list:\n",
    "    if event[-5:]=='Start':\n",
    "        open_event.append(event[:-5])\n",
    "        \n",
    "    elif event[-3:]=='End':\n",
    "        open_event.remove(event[:-3])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The events that are still open is ['SparkListenerLog', 'SparkListenerApplication']\n"
     ]
    }
   ],
   "source": [
    "print(f'The events that are still open is {open_event}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_time_dict=dict()\n",
    "for event in df.collect():\n",
    "    \n",
    "    event_d=event.asDict()\n",
    "    try:\n",
    "        start_time = event_d['Task Info']['Launch Time']\n",
    "        end_time = event_d['Task Info']['Finish Time']\n",
    "        \n",
    "        if end_time > 0:            \n",
    "            try:\n",
    "                event_time_dict[event_d['Event']]+=','+ str(end_time-start_time)\n",
    "            except:\n",
    "                event_time_dict[event_d['Event']]=str(end_time-start_time)\n",
    "    except:\n",
    "        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The task SparkListenerTaskEnd has the average time 238.3846153846154\n"
     ]
    }
   ],
   "source": [
    "for task, time in event_time_dict.items():\n",
    "    \n",
    " s_time=time.split(',')\n",
    "\n",
    " avg=sum(list(map(int,s_time)))/len(s_time)\n",
    " print(f' The task {task} has the average time {avg}')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The long job list is [('SparkListenerTaskEnd', 616), ('SparkListenerTaskEnd', 620), ('SparkListenerTaskEnd', 621), ('SparkListenerTaskEnd', 621), ('SparkListenerTaskEnd', 623), ('SparkListenerTaskEnd', 621), ('SparkListenerTaskEnd', 621), ('SparkListenerTaskEnd', 660), ('SparkListenerTaskEnd', 486)]\n"
     ]
    }
   ],
   "source": [
    "for task, time in event_time_dict.items():\n",
    "    \n",
    " s_time=time.split(',')\n",
    "\n",
    " l_time=list(map(int,s_time))\n",
    "    \n",
    " long_job=[]\n",
    " \n",
    "for e_time in l_time:\n",
    "    if e_time > avg:\n",
    "        long_job.append((task,e_time))\n",
    "        \n",
    "        \n",
    "        \n",
    "print(f'The long job list is {long_job}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaString = \"name age city address postcode\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(age,StringType,true),StructField(city,StringType,true),StructField(address,StringType,true),StructField(postcode,StringType,true)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "# Name: Justin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: Justin']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teenNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
